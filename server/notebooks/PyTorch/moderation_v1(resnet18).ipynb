{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "moderation v1(resnet18).ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1V2g5P88r7WApofQmZl5GVe8YFWi7MGRJ",
      "authorship_tag": "ABX9TyN3i6DlR/BmMUMFUqmt4oQF",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "00cf72284cf64fefb3bc6aefc2cdab4f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_9cdae512af6a490487b80436dfff2f3f",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_78a1d5133a1948c486cc31cd1c8dc498",
              "IPY_MODEL_5838e3cc979549b19af4f6c93affccea"
            ]
          }
        },
        "9cdae512af6a490487b80436dfff2f3f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "78a1d5133a1948c486cc31cd1c8dc498": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_310d3ab9edea46319ef01dfed328ea50",
            "_dom_classes": [],
            "description": "100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 46827520,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 46827520,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_7019a07c7285435f8df40fb6477d39d6"
          }
        },
        "5838e3cc979549b19af4f6c93affccea": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_3358d7d9fb734c14ad5cb05b1e328b18",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 44.7M/44.7M [39:12&lt;00:00, 19.9kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_f1dcdc06073d41f6a0902a244b8c5a24"
          }
        },
        "310d3ab9edea46319ef01dfed328ea50": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "7019a07c7285435f8df40fb6477d39d6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "3358d7d9fb734c14ad5cb05b1e328b18": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "f1dcdc06073d41f6a0902a244b8c5a24": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shreyanshtomar/moderation/blob/shreyansh_dev/server/notebooks/PyTorch/moderation_v1(resnet18).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hke2de6AuPgh",
        "colab_type": "text"
      },
      "source": [
        "# Getting the data...\n",
        "\n",
        "Source: https://www.kaggle.com/omeret/nsfw-nsafe & https://www.kaggle.com/omeret/nsfw-safe.\n",
        "\n",
        "The combined dataset contains: \n",
        "* Number training images:  103518\n",
        "* Num test images:  3365"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hB_-CwzoSJHa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "! pip install -q kaggle"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z_rQWtWaStKP",
        "colab_type": "code",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 92
        },
        "outputId": "ff66a775-46a6-43e9-ad94-e3205c6b43c7"
      },
      "source": [
        "from google.colab import files\n",
        "files.upload()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-f5b753da-fa3c-43a9-bb1e-af36de9e3893\" name=\"files[]\" multiple disabled />\n",
              "     <output id=\"result-f5b753da-fa3c-43a9-bb1e-af36de9e3893\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving kaggle.json to kaggle.json\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'kaggle.json': b'{\"username\":\"shreyanshtomar\",\"key\":\"e573d8377370c57bf874be0ef0084247\"}'}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ro256fr9Sxlv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%bash\n",
        "mkdir ~/.kaggle\n",
        "cp kaggle.json ~/.kaggle/\n",
        "chmod 600 ~/.kaggle/kaggle.json"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gmv4hw_rTJU2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "7a963060-9d5a-41a2-febd-5effaebc3543"
      },
      "source": [
        "!kaggle datasets download -d omeret/nsfw-nsafe"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading nsfw-nsafe.zip to /content\n",
            "100% 11.5G/11.5G [04:39<00:00, 49.3MB/s]\n",
            "100% 11.5G/11.5G [04:39<00:00, 44.3MB/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q_ZxlwQjTvyB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!unzip nsfw-nsafe.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "czz-30qcHrcE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "48f113c9-17a6-4eb9-bfa4-33ce77177132"
      },
      "source": [
        "!kaggle datasets download -d omeret/nsfw-safe"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading nsfw-safe.zip to /content\n",
            "100% 10.3G/10.4G [04:10<00:00, 34.4MB/s]\n",
            "100% 10.4G/10.4G [04:10<00:00, 44.4MB/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A46jWCQQIvsl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!unzip nsfw-safe.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X3TJMBRDvd-k",
        "colab_type": "text"
      },
      "source": [
        "##Importing Library and Data\n",
        "\n",
        "To begin, import the torch and torchvision frameworks and their libraries with numpy, pandas, and sklearn. Libraries and functions used in the code below include:\n",
        "\n",
        "* [transforms](https://pytorch.org/docs/stable/torchvision/transforms.html), for basic image transformation\n",
        "* [torch.nn.functional](https://pytorch.org/docs/stable/nn.functional.html), which contains useful activation functions.\n",
        "* [Dataset and Dataloader](https://pytorch.org/docs/0.3.0/torchvision/datasets.html), PyTorch's data loading utility \n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zX1bhSTjWHgv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import copy\n",
        "import cv2                \n",
        "from glob import glob\n",
        "from io import open\n",
        "import json\n",
        "import matplotlib.image as mpimg\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os, os.path, random\n",
        "from PIL import Image\n",
        "import requests\n",
        "import shutil\n",
        "import time\n",
        "\n",
        "import torch\n",
        "from torch.autograd import Variable\n",
        "import torch.functional as F\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.optim import lr_scheduler\n",
        "import torchvision\n",
        "from torchvision import datasets, models, transforms\n",
        "from torch.utils.data.sampler import SubsetRandomSampler\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "plt.ion()   # interactive mode\n",
        "\n",
        "%matplotlib inline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZxAdEmuXw_q1",
        "colab_type": "text"
      },
      "source": [
        "Specifying the data directories."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oE-LwAndCt8A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data_dir = '/content/'\n",
        "train_dir = os.path.join(data_dir, 'train/')\n",
        "test_dir = os.path.join(data_dir, 'test/')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V539BDqxxD4g",
        "colab_type": "text"
      },
      "source": [
        "#Image Pre-processing\n",
        "\n",
        "Images in a dataset do not usually have the same pixel intensity and dimensions.\n",
        "\n",
        "You can stack multiple image transformation commands in [transform.Compose](https://pytorch.org/docs/master/torchvision/transforms.html#torchvision.transforms.Compose). Normalizing an image is an important step that makes model training stable and fast. In tranforms.Normalize() class, a list of means and standard deviations is sent in the form of a list. It uses this formula: ![alt text](https://i.imgur.com/pWSTFzG.png).\n",
        "<br>\n",
        "## Dataset Split\n",
        "\n",
        "How well the model can learn depends on the variety and volume of the data. We need to divide our data into a training set and a validation set using PyTorch's [datasets.ImageFolder](https://pytorch.org/docs/master/torchvision/datasets.html?highlight=dataset%20image#torchvision.datasets.ImageFolder) utility since we already have downloaded the dataset in train_dir and test_dir directories.\n",
        "\n",
        "**Training dataset**: The model learns from this dataset's examples. It fits a parameter to a classifier.\n",
        "\n",
        "**Validation dataset**: The examples in the validation dataset are used to tune the hyperparameters, such as learning rate and epochs. The aim of creating a validation set is to avoid large overfitting of the model. It is a checkpoint to know if the model is fitted well with the training dataset.\n",
        "\n",
        "Test dataset: This dataset test the final evolution of the model, measuring how well it has learned and predicted the desired output. It contains unseen, real-life data.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pg8C7TzpDJi5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "5ea87948-0ec3-47ff-9284-c91b92ad3a0c"
      },
      "source": [
        "data_transform = transforms.Compose([transforms.RandomResizedCrop(224),\n",
        "                                      transforms.ToTensor(),\n",
        "                                     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "\n",
        "train_data = datasets.ImageFolder(train_dir, transform=data_transform)\n",
        "test_data = datasets.ImageFolder(test_dir, transform=data_transform)\n",
        "\n",
        "# print out some data stats\n",
        "print('Num training images: ', len(train_data))\n",
        "print('Num test images: ', len(test_data))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Num training images:  103518\n",
            "Num test images:  3365\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eefSGdw_wv4K",
        "colab_type": "text"
      },
      "source": [
        "Whenever you initialize the batch of images, it is on the CPU for computation by default. The function torch.cuda.is_available() will check whether a GPU is present. If CUDA is present, .device(\"cuda\") will route the tensor to the GPU for computation.\n",
        "\n",
        "The device will use CUDA with a single GPU processor. This will make our calculations faster. If you have a CPU in your system, no problem. You can use Google Colab, which provides free GPU."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_xo8cr93-tgS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "2d51fea6-a4a8-4491-8455-6f518b6b0c02"
      },
      "source": [
        "# check if CUDA is available\n",
        "device_avail = torch.cuda.is_available()\n",
        "\n",
        "if not device:\n",
        "    print('CUDA is not available.  Training on CPU ...')\n",
        "else:\n",
        "    print('CUDA is available!  Training on GPU ...')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CUDA is available!  Training on GPU ...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QlHuyPb0z2kW",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "In the code below, `dataloader` combines a dataset and a sampler and provides an iterable over the given dataset. `dataset()` indicates which dataset to load form the available data. For details, [read this documentation](https://pytorch.org/docs/stable/data.html#module-torch.utils.data).\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z74c-DT1EBYe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# define dataloader parameters\n",
        "batch_size = 128\n",
        "num_workers=0\n",
        "\n",
        "# prepare data loaders\n",
        "train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, \n",
        "                                           num_workers=num_workers, shuffle=True)\n",
        "test_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size, \n",
        "                                          num_workers=num_workers, shuffle=True)\n",
        "loaders_scratch = {\n",
        "    'train': train_loader,\n",
        "    'test': test_loader\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HWpgkL6JEE0w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Visualize some sample data\n",
        "# obtain one batch of training images\n",
        "dataiter = iter(train_loader)\n",
        "images, labels = dataiter.next()\n",
        "images = images.numpy() # convert images to numpy for display\n",
        "\n",
        "# plot the images in the batch, along with the corresponding labels\n",
        "fig = plt.figure(figsize=(25, 4))\n",
        "for idx in np.arange(20):\n",
        "    ax = fig.add_subplot(2, 20/2, idx+1, xticks=[], yticks=[])\n",
        "    plt.imshow(np.transpose(images[idx], (1, 2, 0)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WrxvUovCGCUt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "7892cc4b-5668-4461-9ead-a823e4b8c240"
      },
      "source": [
        "images, labels = next(iter(train_loader)) \n",
        "print(\"images-size:\", images.shape)\n",
        "\n",
        "out = torchvision.utils.make_grid(images)\n",
        "print(\"out-size:\", out.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "images-size: torch.Size([128, 3, 224, 224])\n",
            "out-size: torch.Size([3, 3618, 1810])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_hqALVbP1WWX",
        "colab_type": "text"
      },
      "source": [
        "#Transfer Learning with Pytorch\n",
        "\n",
        "The main aim of transfer learning (TL) is to implement a model quickly. To solve the current problem, instead of creating a DNN (dense neural network) from scratch, the model will transfer the features it has learned from the different dataset that has performed the same task. This transaction is also known as knowledge transfer.\n",
        "\n",
        "<img src=\"https://i.imgur.com/3sx8Y3i.png\" width = \"600\"/>\n",
        "\n",
        "Why Resnet-18 ? \n",
        "\n",
        "ResNet-18 is a convolutional neural network that is trained on more than a million images from the ImageNet database. There are 18 layers present in its architecture. It is `very useful and efficient in image classification` and can classify images into 1000 object categories. The network has an image input size of 224x224.\n",
        "Also,\n",
        "1. Networks with large number (even thousands) of layers can be trained easily without increasing the training error percentage.\n",
        "2. ResNets help in tackling the vanishing gradient problem using identity mapping.\n",
        "\n",
        "The Pytorch API calls a pre-trained model of ResNet18 by using models.resnet18(pretrained=True), the function from TorchVision's model library. ResNet-18 architecture is described below.\n",
        "<img src=\"https://i.imgur.com/XwcnU5x.png\" width = \"600\"/>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "un6P_1F8EG7A",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "00cf72284cf64fefb3bc6aefc2cdab4f",
            "9cdae512af6a490487b80436dfff2f3f",
            "78a1d5133a1948c486cc31cd1c8dc498",
            "5838e3cc979549b19af4f6c93affccea",
            "310d3ab9edea46319ef01dfed328ea50",
            "7019a07c7285435f8df40fb6477d39d6",
            "3358d7d9fb734c14ad5cb05b1e328b18",
            "f1dcdc06073d41f6a0902a244b8c5a24"
          ]
        },
        "outputId": "e0112dc2-f9d8-49b5-b453-e04089a16b1b"
      },
      "source": [
        "net = models.resnet18(pretrained=True)\n",
        "net = net.cuda() if device else net\n",
        "net"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/resnet18-5c106cde.pth\" to /root/.cache/torch/checkpoints/resnet18-5c106cde.pth\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "00cf72284cf64fefb3bc6aefc2cdab4f",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=46827520.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ResNet(\n",
              "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
              "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (relu): ReLU(inplace=True)\n",
              "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
              "  (layer1): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (layer2): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (layer3): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (layer4): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
              "  (fc): Linear(in_features=512, out_features=1000, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rYOjWpfdG6WW",
        "colab_type": "text"
      },
      "source": [
        "## Specifying Loss & Optimizer:\n",
        "\n",
        "Loss function and optimization go hand-in-hand. Loss function checks whether the model is moving in the correct direction and making progress, whereas optimization improves the model to deliver accurate results.\n",
        "Select any one optimizer algorithm available in the [torch.optim](https://pytorch.org/docs/master/optim.html) package. The optimizers have some elements of the gradient descent.\n",
        "\n",
        "Also, setting the hyperparameters below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ypffh-IQFGpy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(net.parameters(), lr=0.0001, momentum=0.9)\n",
        "\n",
        "def accuracy(out, labels):\n",
        "    _,pred = torch.max(out, dim=1)\n",
        "    return torch.sum(pred==labels).item()\n",
        "\n",
        "num_ftrs = net.fc.in_features\n",
        "net.fc = nn.Linear(num_ftrs, 128)\n",
        "net.fc = net.fc.cuda() if device else net.fc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kTxfIfk1Ie4S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F5pBMdSCHBYB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "a02670af-88c9-4223-caf8-0951d27bcffb"
      },
      "source": [
        "n_epochs = 5\n",
        "print_every = 10\n",
        "valid_loss_min = np.Inf\n",
        "val_loss = [] \n",
        "val_acc = []\n",
        "train_loss = []\n",
        "train_acc = []\n",
        "total_step = len(train_loader)\n",
        "\n",
        "    ###################\n",
        "    # train the model #\n",
        "    ###################\n",
        "for epoch in range(1, n_epochs+1):\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total=0\n",
        "    print(f'Epoch {epoch}\\n')\n",
        "    for batch_idx, (data_, target_) in enumerate(train_loader):\n",
        "        data_, target_ = data_.to(device), target_.to(device)\n",
        "        optimizer.zero_grad() # clear-the-gradients-of-all-optimized-variables\n",
        "        \n",
        "        outputs = net(data_) # forward-pass: compute-predicted-outputs-by-passing-inputs-to-the-model\n",
        "        loss = criterion(outputs, target_) # calculate-the-batch-loss\n",
        "        loss.backward()  # backward-pass: compute-gradient-of-the-loss-wrt-model-parameters\n",
        "        optimizer.step()  # perform-a-ingle-optimization-step (parameter-update)\n",
        "\n",
        "        running_loss += loss.item() # update-training-loss\n",
        "        _,pred = torch.max(outputs, dim=1)\n",
        "        correct += torch.sum(pred==target_).item()\n",
        "        total += target_.size(0)\n",
        "        if (batch_idx) % 20 == 0:\n",
        "            print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}' \n",
        "                   .format(epoch, n_epochs, batch_idx, total_step, loss.item()))\n",
        "    train_acc.append(100 * correct / total)\n",
        "    train_loss.append(running_loss/total_step)\n",
        "    print(f'\\ntrain-loss: {np.mean(train_loss):.4f}, train-acc: {(100 * correct/total):.4f}')\n",
        "    batch_loss = 0\n",
        "    total_t=0\n",
        "    correct_t=0\n",
        "\n",
        "    ######################    \n",
        "    # validate the model #\n",
        "    ######################\n",
        "    with torch.no_grad():\n",
        "        net.eval()\n",
        "        for data_t, target_t in (test_loader):\n",
        "            data_t, target_t = data_t.to(device), target_t.to(device)\n",
        "            outputs_t = net(data_t) # forward-pass: compute-predicted-outputs-by-passing-inputs-to-the-model\n",
        "            loss_t = criterion(outputs_t, target_t) # calculate-the-batch-loss\n",
        "            batch_loss += loss_t.item()  # update-average-validation-loss \n",
        "            _,pred_t = torch.max(outputs_t, dim=1)\n",
        "            correct_t += torch.sum(pred_t==target_t).item()\n",
        "            total_t += target_t.size(0)\n",
        "        val_acc.append(100 * correct_t/total_t)\n",
        "        val_loss.append(batch_loss/len(test_loader)) \n",
        "        network_learned = batch_loss < valid_loss_min\n",
        "        print(f'validation loss: {np.mean(val_loss):.4f}, validation acc: {(100 * correct_t/total_t):.4f}\\n')\n",
        "\n",
        "        \n",
        "        if network_learned:\n",
        "            valid_loss_min = batch_loss\n",
        "            torch.save(net.state_dict(), 'resnet.pt')\n",
        "            print('Improvement-Detected, save-model')\n",
        "    net.train()\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1\n",
            "\n",
            "Epoch [1/5], Step [0/809], Loss: 5.2429\n",
            "Epoch [1/5], Step [20/809], Loss: 5.2985\n",
            "Epoch [1/5], Step [40/809], Loss: 5.2799\n",
            "Epoch [1/5], Step [60/809], Loss: 5.1616\n",
            "Epoch [1/5], Step [80/809], Loss: 5.2010\n",
            "Epoch [1/5], Step [100/809], Loss: 4.9265\n",
            "Epoch [1/5], Step [120/809], Loss: 4.7991\n",
            "Epoch [1/5], Step [140/809], Loss: 4.7754\n",
            "Epoch [1/5], Step [160/809], Loss: 4.6035\n",
            "Epoch [1/5], Step [180/809], Loss: 4.6426\n",
            "Epoch [1/5], Step [200/809], Loss: 4.4863\n",
            "Epoch [1/5], Step [220/809], Loss: 4.3222\n",
            "Epoch [1/5], Step [240/809], Loss: 4.3232\n",
            "Epoch [1/5], Step [260/809], Loss: 4.2281\n",
            "Epoch [1/5], Step [280/809], Loss: 3.9456\n",
            "Epoch [1/5], Step [300/809], Loss: 3.9685\n",
            "Epoch [1/5], Step [320/809], Loss: 3.8650\n",
            "Epoch [1/5], Step [340/809], Loss: 3.8362\n",
            "Epoch [1/5], Step [360/809], Loss: 3.5822\n",
            "Epoch [1/5], Step [380/809], Loss: 3.3608\n",
            "Epoch [1/5], Step [400/809], Loss: 3.5758\n",
            "Epoch [1/5], Step [420/809], Loss: 3.2884\n",
            "Epoch [1/5], Step [440/809], Loss: 3.1906\n",
            "Epoch [1/5], Step [460/809], Loss: 3.1809\n",
            "Epoch [1/5], Step [480/809], Loss: 3.2350\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/PIL/TiffImagePlugin.py:788: UserWarning: Corrupt EXIF data.  Expecting to read 2 bytes but only got 0. \n",
            "  warnings.warn(str(msg))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch [1/5], Step [500/809], Loss: 2.9568\n",
            "Epoch [1/5], Step [520/809], Loss: 2.6109\n",
            "Epoch [1/5], Step [540/809], Loss: 2.8598\n",
            "Epoch [1/5], Step [560/809], Loss: 2.2944\n",
            "Epoch [1/5], Step [580/809], Loss: 2.6249\n",
            "Epoch [1/5], Step [600/809], Loss: 2.5698\n",
            "Epoch [1/5], Step [620/809], Loss: 2.2766\n",
            "Epoch [1/5], Step [640/809], Loss: 2.2526\n",
            "Epoch [1/5], Step [660/809], Loss: 2.2072\n",
            "Epoch [1/5], Step [680/809], Loss: 2.1748\n",
            "Epoch [1/5], Step [700/809], Loss: 2.0582\n",
            "Epoch [1/5], Step [720/809], Loss: 2.3753\n",
            "Epoch [1/5], Step [740/809], Loss: 1.8946\n",
            "Epoch [1/5], Step [760/809], Loss: 1.7379\n",
            "Epoch [1/5], Step [780/809], Loss: 1.7760\n",
            "Epoch [1/5], Step [800/809], Loss: 1.7112\n",
            "\n",
            "train-loss: 3.4795, train-acc: 47.8004\n",
            "validation loss: 1.8241, validation acc: 86.9242\n",
            "\n",
            "Improvement-Detected, save-model\n",
            "Epoch 2\n",
            "\n",
            "Epoch [2/5], Step [0/809], Loss: 1.6534\n",
            "Epoch [2/5], Step [20/809], Loss: 2.0337\n",
            "Epoch [2/5], Step [40/809], Loss: 1.6541\n",
            "Epoch [2/5], Step [60/809], Loss: 1.8867\n",
            "Epoch [2/5], Step [80/809], Loss: 1.6864\n",
            "Epoch [2/5], Step [100/809], Loss: 1.5583\n",
            "Epoch [2/5], Step [120/809], Loss: 1.8476\n",
            "Epoch [2/5], Step [140/809], Loss: 1.9611\n",
            "Epoch [2/5], Step [160/809], Loss: 1.5778\n",
            "Epoch [2/5], Step [180/809], Loss: 1.7341\n",
            "Epoch [2/5], Step [200/809], Loss: 1.6033\n",
            "Epoch [2/5], Step [220/809], Loss: 1.3731\n",
            "Epoch [2/5], Step [240/809], Loss: 1.8699\n",
            "Epoch [2/5], Step [260/809], Loss: 1.5715\n",
            "Epoch [2/5], Step [280/809], Loss: 1.4847\n",
            "Epoch [2/5], Step [300/809], Loss: 1.3364\n",
            "Epoch [2/5], Step [320/809], Loss: 1.4751\n",
            "Epoch [2/5], Step [340/809], Loss: 1.5692\n",
            "Epoch [2/5], Step [360/809], Loss: 1.6405\n",
            "Epoch [2/5], Step [380/809], Loss: 1.2659\n",
            "Epoch [2/5], Step [400/809], Loss: 1.3667\n",
            "Epoch [2/5], Step [420/809], Loss: 1.3793\n",
            "Epoch [2/5], Step [440/809], Loss: 1.2604\n",
            "Epoch [2/5], Step [460/809], Loss: 1.3330\n",
            "Epoch [2/5], Step [480/809], Loss: 1.1316\n",
            "Epoch [2/5], Step [500/809], Loss: 1.5569\n",
            "Epoch [2/5], Step [520/809], Loss: 1.4590\n",
            "Epoch [2/5], Step [540/809], Loss: 1.6933\n",
            "Epoch [2/5], Step [560/809], Loss: 1.1233\n",
            "Epoch [2/5], Step [580/809], Loss: 1.4707\n",
            "Epoch [2/5], Step [600/809], Loss: 1.2190\n",
            "Epoch [2/5], Step [620/809], Loss: 1.3310\n",
            "Epoch [2/5], Step [640/809], Loss: 1.4667\n",
            "Epoch [2/5], Step [660/809], Loss: 1.2820\n",
            "Epoch [2/5], Step [680/809], Loss: 1.0175\n",
            "Epoch [2/5], Step [700/809], Loss: 1.3047\n",
            "Epoch [2/5], Step [720/809], Loss: 1.2643\n",
            "Epoch [2/5], Step [740/809], Loss: 1.5686\n",
            "Epoch [2/5], Step [760/809], Loss: 1.4757\n",
            "Epoch [2/5], Step [780/809], Loss: 0.9641\n",
            "Epoch [2/5], Step [800/809], Loss: 1.0994\n",
            "\n",
            "train-loss: 2.4576, train-acc: 89.3951\n",
            "validation loss: 1.4326, validation acc: 91.6493\n",
            "\n",
            "Improvement-Detected, save-model\n",
            "Epoch 3\n",
            "\n",
            "Epoch [3/5], Step [0/809], Loss: 1.1954\n",
            "Epoch [3/5], Step [20/809], Loss: 1.0953\n",
            "Epoch [3/5], Step [40/809], Loss: 1.2402\n",
            "Epoch [3/5], Step [60/809], Loss: 1.7142\n",
            "Epoch [3/5], Step [80/809], Loss: 1.6022\n",
            "Epoch [3/5], Step [100/809], Loss: 1.0044\n",
            "Epoch [3/5], Step [120/809], Loss: 1.0165\n",
            "Epoch [3/5], Step [140/809], Loss: 0.8158\n",
            "Epoch [3/5], Step [160/809], Loss: 0.9924\n",
            "Epoch [3/5], Step [180/809], Loss: 0.9026\n",
            "Epoch [3/5], Step [200/809], Loss: 1.0213\n",
            "Epoch [3/5], Step [220/809], Loss: 1.1822\n",
            "Epoch [3/5], Step [240/809], Loss: 1.2278\n",
            "Epoch [3/5], Step [260/809], Loss: 0.9604\n",
            "Epoch [3/5], Step [280/809], Loss: 0.8093\n",
            "Epoch [3/5], Step [300/809], Loss: 0.9142\n",
            "Epoch [3/5], Step [320/809], Loss: 1.0818\n",
            "Epoch [3/5], Step [340/809], Loss: 1.0016\n",
            "Epoch [3/5], Step [360/809], Loss: 1.1238\n",
            "Epoch [3/5], Step [380/809], Loss: 0.7568\n",
            "Epoch [3/5], Step [400/809], Loss: 0.8309\n",
            "Epoch [3/5], Step [420/809], Loss: 1.0277\n",
            "Epoch [3/5], Step [440/809], Loss: 0.5757\n",
            "Epoch [3/5], Step [460/809], Loss: 1.3167\n",
            "Epoch [3/5], Step [480/809], Loss: 1.7649\n",
            "Epoch [3/5], Step [500/809], Loss: 0.9445\n",
            "Epoch [3/5], Step [520/809], Loss: 0.7854\n",
            "Epoch [3/5], Step [540/809], Loss: 0.7613\n",
            "Epoch [3/5], Step [560/809], Loss: 1.0804\n",
            "Epoch [3/5], Step [580/809], Loss: 0.9315\n",
            "Epoch [3/5], Step [600/809], Loss: 0.9577\n",
            "Epoch [3/5], Step [620/809], Loss: 0.6854\n",
            "Epoch [3/5], Step [640/809], Loss: 0.8866\n",
            "Epoch [3/5], Step [660/809], Loss: 0.8707\n",
            "Epoch [3/5], Step [680/809], Loss: 0.7814\n",
            "Epoch [3/5], Step [700/809], Loss: 1.1083\n",
            "Epoch [3/5], Step [720/809], Loss: 0.8456\n",
            "Epoch [3/5], Step [740/809], Loss: 1.0168\n",
            "Epoch [3/5], Step [760/809], Loss: 0.7021\n",
            "Epoch [3/5], Step [780/809], Loss: 0.6537\n",
            "Epoch [3/5], Step [800/809], Loss: 0.8092\n",
            "\n",
            "train-loss: 1.9668, train-acc: 90.9108\n",
            "validation loss: 1.2205, validation acc: 92.1842\n",
            "\n",
            "Improvement-Detected, save-model\n",
            "Epoch 4\n",
            "\n",
            "Epoch [4/5], Step [0/809], Loss: 0.7946\n",
            "Epoch [4/5], Step [20/809], Loss: 1.0308\n",
            "Epoch [4/5], Step [40/809], Loss: 0.6560\n",
            "Epoch [4/5], Step [60/809], Loss: 1.0302\n",
            "Epoch [4/5], Step [80/809], Loss: 0.8592\n",
            "Epoch [4/5], Step [100/809], Loss: 0.8019\n",
            "Epoch [4/5], Step [120/809], Loss: 1.0326\n",
            "Epoch [4/5], Step [140/809], Loss: 0.9286\n",
            "Epoch [4/5], Step [160/809], Loss: 0.8285\n",
            "Epoch [4/5], Step [180/809], Loss: 0.4850\n",
            "Epoch [4/5], Step [200/809], Loss: 0.8094\n",
            "Epoch [4/5], Step [220/809], Loss: 0.7464\n",
            "Epoch [4/5], Step [240/809], Loss: 0.6709\n",
            "Epoch [4/5], Step [260/809], Loss: 0.8356\n",
            "Epoch [4/5], Step [280/809], Loss: 0.8476\n",
            "Epoch [4/5], Step [300/809], Loss: 0.8821\n",
            "Epoch [4/5], Step [320/809], Loss: 0.7920\n",
            "Epoch [4/5], Step [340/809], Loss: 1.0616\n",
            "Epoch [4/5], Step [360/809], Loss: 0.7893\n",
            "Epoch [4/5], Step [380/809], Loss: 0.5901\n",
            "Epoch [4/5], Step [400/809], Loss: 0.8967\n",
            "Epoch [4/5], Step [420/809], Loss: 0.6820\n",
            "Epoch [4/5], Step [440/809], Loss: 0.6955\n",
            "Epoch [4/5], Step [460/809], Loss: 0.9195\n",
            "Epoch [4/5], Step [480/809], Loss: 0.6592\n",
            "Epoch [4/5], Step [500/809], Loss: 0.8411\n",
            "Epoch [4/5], Step [520/809], Loss: 0.5146\n",
            "Epoch [4/5], Step [540/809], Loss: 0.4799\n",
            "Epoch [4/5], Step [560/809], Loss: 0.7221\n",
            "Epoch [4/5], Step [580/809], Loss: 0.9450\n",
            "Epoch [4/5], Step [600/809], Loss: 0.6284\n",
            "Epoch [4/5], Step [620/809], Loss: 0.9854\n",
            "Epoch [4/5], Step [640/809], Loss: 0.9246\n",
            "Epoch [4/5], Step [660/809], Loss: 0.6525\n",
            "Epoch [4/5], Step [680/809], Loss: 0.6038\n",
            "Epoch [4/5], Step [700/809], Loss: 0.8352\n",
            "Epoch [4/5], Step [720/809], Loss: 0.5996\n",
            "Epoch [4/5], Step [740/809], Loss: 0.6594\n",
            "Epoch [4/5], Step [760/809], Loss: 0.8440\n",
            "Epoch [4/5], Step [780/809], Loss: 0.7428\n",
            "Epoch [4/5], Step [800/809], Loss: 0.6213\n",
            "\n",
            "train-loss: 1.6744, train-acc: 91.5184\n",
            "validation loss: 1.0787, validation acc: 92.6597\n",
            "\n",
            "Improvement-Detected, save-model\n",
            "Epoch 5\n",
            "\n",
            "Epoch [5/5], Step [0/809], Loss: 0.6859\n",
            "Epoch [5/5], Step [20/809], Loss: 0.6036\n",
            "Epoch [5/5], Step [40/809], Loss: 0.7785\n",
            "Epoch [5/5], Step [60/809], Loss: 0.8621\n",
            "Epoch [5/5], Step [80/809], Loss: 1.1127\n",
            "Epoch [5/5], Step [100/809], Loss: 0.7347\n",
            "Epoch [5/5], Step [120/809], Loss: 0.8378\n",
            "Epoch [5/5], Step [140/809], Loss: 0.6052\n",
            "Epoch [5/5], Step [160/809], Loss: 0.6814\n",
            "Epoch [5/5], Step [180/809], Loss: 0.7446\n",
            "Epoch [5/5], Step [200/809], Loss: 0.7204\n",
            "Epoch [5/5], Step [220/809], Loss: 0.7752\n",
            "Epoch [5/5], Step [240/809], Loss: 1.0248\n",
            "Epoch [5/5], Step [260/809], Loss: 0.7691\n",
            "Epoch [5/5], Step [280/809], Loss: 0.7775\n",
            "Epoch [5/5], Step [300/809], Loss: 0.5314\n",
            "Epoch [5/5], Step [320/809], Loss: 0.5186\n",
            "Epoch [5/5], Step [340/809], Loss: 0.7084\n",
            "Epoch [5/5], Step [360/809], Loss: 0.6074\n",
            "Epoch [5/5], Step [380/809], Loss: 0.6271\n",
            "Epoch [5/5], Step [400/809], Loss: 0.8706\n",
            "Epoch [5/5], Step [420/809], Loss: 0.8522\n",
            "Epoch [5/5], Step [440/809], Loss: 0.6487\n",
            "Epoch [5/5], Step [460/809], Loss: 0.5331\n",
            "Epoch [5/5], Step [480/809], Loss: 0.7103\n",
            "Epoch [5/5], Step [500/809], Loss: 0.7690\n",
            "Epoch [5/5], Step [520/809], Loss: 0.5612\n",
            "Epoch [5/5], Step [540/809], Loss: 0.7331\n",
            "Epoch [5/5], Step [560/809], Loss: 0.7494\n",
            "Epoch [5/5], Step [580/809], Loss: 0.6512\n",
            "Epoch [5/5], Step [600/809], Loss: 0.7907\n",
            "Epoch [5/5], Step [620/809], Loss: 0.4832\n",
            "Epoch [5/5], Step [640/809], Loss: 0.5981\n",
            "Epoch [5/5], Step [660/809], Loss: 0.5856\n",
            "Epoch [5/5], Step [680/809], Loss: 0.5081\n",
            "Epoch [5/5], Step [700/809], Loss: 0.5495\n",
            "Epoch [5/5], Step [720/809], Loss: 0.6202\n",
            "Epoch [5/5], Step [740/809], Loss: 0.6070\n",
            "Epoch [5/5], Step [760/809], Loss: 0.7167\n",
            "Epoch [5/5], Step [780/809], Loss: 0.5835\n",
            "Epoch [5/5], Step [800/809], Loss: 0.4842\n",
            "\n",
            "train-loss: 1.4790, train-acc: 91.7222\n",
            "validation loss: 0.9832, validation acc: 92.4814\n",
            "\n",
            "Improvement-Detected, save-model\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-29-e73bd225c9a2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     55\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Improvement-Detected, save-model'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m     \u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m \u001b[0mpython\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'python' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9inxX_4UMb1R",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "outputId": "7fde22c0-3e11-444b-fbba-91dcd7fa26ab"
      },
      "source": [
        "def test(loaders, model, criterion, train_on_gpu):\n",
        "\n",
        "    # monitor test loss and accuracy\n",
        "    test_loss = 0.\n",
        "    correct = 0.\n",
        "    total = 0.\n",
        "\n",
        "    for batch_idx, (data, target) in enumerate(loaders['test']):\n",
        "        # move to GPU\n",
        "        if train_on_gpu:\n",
        "            data, target = data.cuda(), target.cuda()\n",
        "        # forward pass: compute predicted outputs by passing inputs to the model\n",
        "        output = model(data)\n",
        "        # calculate the loss\n",
        "        loss = criterion(output, target)\n",
        "        # update average test loss \n",
        "        test_loss = test_loss + ((1 / (batch_idx + 1)) * (loss.data - test_loss))\n",
        "        # convert output probabilities to predicted class\n",
        "        pred = output.data.max(1, keepdim=True)[1]\n",
        "        # compare predictions to true label\n",
        "        correct += np.sum(np.squeeze(pred.eq(target.data.view_as(pred))).cpu().numpy())\n",
        "        total += data.size(0)\n",
        "\n",
        "    print('Test Loss: {:.6f}\\n'.format(test_loss))\n",
        "\n",
        "    print('\\nTest Accuracy: %2d%% (%2d/%2d)' % (\n",
        "        100. * correct / total, correct, total))\n",
        "\n",
        "# call test function    \n",
        "test(loaders_scratch, net, criterion, device)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test Loss: 0.645461\n",
            "\n",
            "\n",
            "Test Accuracy: 91% (3072/3365)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C8auqEd60exz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from PIL import Image\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "def load_input_image(img_path):    \n",
        "    image = Image.open(img_path).convert('RGB')\n",
        "    prediction_transform = transforms.Compose([transforms.Resize(size=(224, 224)),\n",
        "                                     transforms.ToTensor(), \n",
        "                                     standard_normalization])\n",
        "\n",
        "    # discard the transparent, alpha channel (that's the :3) and add the batch dimension\n",
        "    image = prediction_transform(image)[:3,:,:].unsqueeze(0)\n",
        "    return image"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vaao8vU16wmv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "loaders_transfer = loaders_scratch.copy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DS1LX-8E6cL3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class_names = [item[4:].replace(\"_\", \" \") for item in loaders_transfer['train'].dataset.classes]\n",
        "\n",
        "\n",
        "def predict_nsfw(model, class_names, img_path):\n",
        "    # load the image and return the predicted breed\n",
        "    img = load_input_image(img_path)\n",
        "    model = model.cpu()\n",
        "    model.eval()\n",
        "    idx = torch.argmax(model(img))\n",
        "    return class_names[idx]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CZv3QzQ98bkA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_checkpoint(filepath):\n",
        "    checkpoint = torch.load(filepath)\n",
        "    model = fc_model.Network(checkpoint['input_size'],\n",
        "                             checkpoint['output_size'],\n",
        "                             checkpoint['hidden_layers'])\n",
        "    model.load_state_dict(checkpoint['state_dict'])\n",
        "    \n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B4lHvyofAv-e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "torch.save(net.state_dict(), 'resnet18_checkpoint.pth')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fmY1KssyA9o1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "d5e462bf-bac8-4f32-a0e4-01f74aa24911"
      },
      "source": [
        "net.load_state_dict(torch.load('resnet18_checkpoint.pth'))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hhS37x6XG5NO",
        "colab_type": "text"
      },
      "source": [
        "##Preparing the image for inference\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qMdlGUgAFxwm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import io\n",
        "\n",
        "import torchvision.transforms as transforms\n",
        "from PIL import Image\n",
        "\n",
        "def transform_image(image_bytes):\n",
        "    my_transforms = transforms.Compose([transforms.RandomResizedCrop(224),\n",
        "                                        transforms.ToTensor(),\n",
        "                                        transforms.Normalize(\n",
        "                                            [0.5, 0.5, 0.5],\n",
        "                                            [0.5, 0.5, 0.5])])\n",
        "    image = Image.open(io.BytesIO(image_bytes))\n",
        "    return my_transforms(image).unsqueeze(0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HPWeLL4HHCwq",
        "colab_type": "text"
      },
      "source": [
        "*The above method takes image data in bytes, applies the series of transforms and returns a tensor.*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0uWuyK2bHISP",
        "colab_type": "text"
      },
      "source": [
        "Testing the above method.."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lpdhNGwRG2rE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with open(\"/content/park.jpeg\", 'rb') as f:\n",
        "    image_bytes = f.read()\n",
        "    tensor = transform_image(image_bytes=image_bytes)\n",
        "    print(tensor)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pgwXS9sJFQeS",
        "colab_type": "text"
      },
      "source": [
        "#Inference"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WXkN23QKFP8I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!cp '/content/drive/My Drive/fastai_chkpts/resnet18_checkpoint.pth' '/content/' # Getting checkpoint in the ./content directory"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7qDxFDWmtDgC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "0821c2c9-25a1-4510-fb70-9604531ab549"
      },
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "device"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cpu')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AtqUHtj-ta1d",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "8c40de43-29e2-4ae1-d97b-5ea0e8df6d13"
      },
      "source": [
        "# check if CUDA is available\n",
        "device_avail = torch.cuda.is_available()\n",
        "\n",
        "if not device_avail:\n",
        "    print('CUDA is not available.  Training on CPU ...')\n",
        "else:\n",
        "    print('CUDA is available!  Training on GPU ...')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CUDA is not available.  Training on CPU ...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BqLSaMJ8GFYH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "net = models.resnet18(pretrained=True)\n",
        "#net = net.cuda() if device_avail else net.cpu()\n",
        "net = net.to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j05BOXBNGNWm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(net.parameters(), lr=0.0001, momentum=0.9)\n",
        "\n",
        "num_ftrs = net.fc.in_features\n",
        "net.fc = nn.Linear(num_ftrs, 128)\n",
        "net.fc = net.fc.to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jmGM3EzpBs05",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if not device_avail:\n",
        "    net.load_state_dict(torch.load('/content/resnet18_checkpoint.pth', map_location=torch.device('cpu')))\n",
        "else:\n",
        "    net.load_state_dict(torch.load('/content/resnet18_checkpoint.pth'))\n",
        "net.eval()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nUiMY926Hgy7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_prediction(image_bytes):\n",
        "    tensor = transform_image(image_bytes=image_bytes)\n",
        "    tensor = tensor.to(device) #Moving tensor to the available device\n",
        "    outputs = net.forward(tensor)\n",
        "    _, y_hat = outputs.max(1)\n",
        "    return y_hat"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gyAOjWwpH4T2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "4c569cf1-2e70-4723-a1bf-3f48c7b1bb3a"
      },
      "source": [
        "with open(\"/content/park.jpeg\", 'rb') as f:\n",
        "    image_bytes = f.read()\n",
        "    print(get_prediction(image_bytes=image_bytes))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([1])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DKqTZx-4Ipi8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "0a03107d-09b5-405c-d24f-eabe3310b561"
      },
      "source": [
        "with open(\"/content/test.jpg\", 'rb') as f:\n",
        "    image_bytes = f.read()\n",
        "    print(get_prediction(image_bytes=image_bytes))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([0])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mdNV0f9lolC8",
        "colab_type": "text"
      },
      "source": [
        "*The tensor y_hat will contain the index of the predicted class id. However, we need a human readable class name. For that we need a class id to name.*\n",
        "\n",
        "**Therefore, writing the get_prediction function again...**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nsZ6KKcPkt0x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class_index = {0: 'nsfw', 1: 'sfw'}\n",
        "\n",
        "def get_prediction(image_bytes):\n",
        "    tensor = transform_image(image_bytes=image_bytes).to(device)\n",
        "    outputs = net.forward(tensor)\n",
        "    _, y_hat = outputs.max(1)\n",
        "    predicted_idx = y_hat.item()\n",
        "    return class_index[predicted_idx]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VEVr1J_bmGs5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "50933411-b842-42b2-a2d7-e302e09bde13"
      },
      "source": [
        "%time\n",
        "with open(\"/content/park.jpeg\", 'rb') as f:\n",
        "    image_bytes = f.read()\n",
        "    print(get_prediction(image_bytes=image_bytes))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 2 Âµs, sys: 0 ns, total: 2 Âµs\n",
            "Wall time: 5.25 Âµs\n",
            "sfw\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HnSEzOLan5KA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "f9696363-dd23-4698-adb2-16f176aae86d"
      },
      "source": [
        "%time\n",
        "with open(\"/content/test.jpg\", 'rb') as f:\n",
        "    image_bytes = f.read()\n",
        "    print(get_prediction(image_bytes=image_bytes))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 2 Âµs, sys: 1e+03 ns, total: 3 Âµs\n",
            "Wall time: 4.29 Âµs\n",
            "nsfw\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mkeWyUwYsl2Z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
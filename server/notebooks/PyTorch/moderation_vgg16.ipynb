{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "moderation_v1(vgg16).ipynb",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shreyanshtomar/moderation/blob/shreyansh_dev/server/notebooks/PyTorch/moderation_vgg16.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KyzTqfT-__4B",
        "colab_type": "text"
      },
      "source": [
        "# Getting the data...\n",
        "\n",
        "Source: https://www.kaggle.com/omeret/nsfw-nsafe & https://www.kaggle.com/omeret/nsfw-safe.\n",
        "\n",
        "The combined dataset contains: \n",
        "* Number training images:  103518\n",
        "* Num test images:  3365"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gP4Q4efJAFy3",
        "colab_type": "text"
      },
      "source": [
        "After getting the data...\n",
        "##Importing Library and Data\n",
        "\n",
        "To begin, import the torch and torchvision frameworks and their libraries with numpy, pandas, and sklearn. Libraries and functions used in the code below include:\n",
        "\n",
        "* [transforms](https://pytorch.org/docs/stable/torchvision/transforms.html), for basic image transformation\n",
        "* [torch.nn.functional](https://pytorch.org/docs/stable/nn.functional.html), which contains useful activation functions.\n",
        "* [Dataset and Dataloader](https://pytorch.org/docs/0.3.0/torchvision/datasets.html), PyTorch's data loading utility \n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zX1bhSTjWHgv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import copy\n",
        "import cv2                \n",
        "from glob import glob\n",
        "from io import open\n",
        "import json\n",
        "import matplotlib.image as mpimg\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os, os.path, random\n",
        "from PIL import Image\n",
        "import requests\n",
        "import shutil\n",
        "import time\n",
        "\n",
        "import torch\n",
        "from torch.autograd import Variable\n",
        "import torch.functional as F\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.optim import lr_scheduler\n",
        "import torchvision\n",
        "from torchvision import datasets, models, transforms\n",
        "\n",
        "plt.ion()   # interactive mode\n",
        "\n",
        "%matplotlib inline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YyOVUOVIAf9u",
        "colab_type": "text"
      },
      "source": [
        "Specifying the data directories."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oE-LwAndCt8A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data_dir = '/content/'\n",
        "train_dir = os.path.join(data_dir, 'train/')\n",
        "test_dir = os.path.join(data_dir, 'test/')\n",
        "\n",
        "# classes are folders in each directory with these names\n",
        "classes = ['nsfw','sfw']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fjNBuP_yAU8e",
        "colab_type": "text"
      },
      "source": [
        "#Image Pre-processing\n",
        "\n",
        "Images in a dataset do not usually have the same pixel intensity and dimensions.\n",
        "\n",
        "You can stack multiple image transformation commands in [transform.Compose](https://pytorch.org/docs/master/torchvision/transforms.html#torchvision.transforms.Compose). Normalizing an image is an important step that makes model training stable and fast. In tranforms.Normalize() class, a list of means and standard deviations is sent in the form of a list. It uses this formula: ![alt text](https://i.imgur.com/pWSTFzG.png).\n",
        "<br>\n",
        "## Dataset Split\n",
        "\n",
        "How well the model can learn depends on the variety and volume of the data. We need to divide our data into a training set and a validation set using PyTorch's [datasets.ImageFolder](https://pytorch.org/docs/master/torchvision/datasets.html?highlight=dataset%20image#torchvision.datasets.ImageFolder) utility since we already have downloaded the dataset in train_dir and test_dir directories.\n",
        "\n",
        "**Training dataset**: The model learns from this dataset's examples. It fits a parameter to a classifier.\n",
        "\n",
        "**Validation dataset**: The examples in the validation dataset are used to tune the hyperparameters, such as learning rate and epochs. The aim of creating a validation set is to avoid large overfitting of the model. It is a checkpoint to know if the model is fitted well with the training dataset.\n",
        "\n",
        "Test dataset: This dataset test the final evolution of the model, measuring how well it has learned and predicted the desired output. It contains unseen, real-life data.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pg8C7TzpDJi5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "1d739935-061e-4634-966c-bd509e5d7d33"
      },
      "source": [
        "train_transforms = transforms.Compose([\n",
        "                            transforms.RandomResizedCrop(size=224),                                   \n",
        "                            transforms.RandomHorizontalFlip(),\n",
        "                            transforms.ToTensor(),\n",
        "                            transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))])\n",
        "\n",
        "test_transforms = transforms.Compose([                                  \n",
        "                            transforms.Resize(256),\n",
        "                            transforms.CenterCrop(224),\n",
        "                            transforms.ToTensor(),\n",
        "                            transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))])\n",
        "\n",
        "\n",
        "train_data = datasets.ImageFolder(train_dir, transform=train_transforms)\n",
        "test_data = datasets.ImageFolder(test_dir, transform=test_transforms)\n",
        "\n",
        "# print out some data stats\n",
        "print('Num training images: ', len(train_data))\n",
        "print('Num test images: ', len(test_data))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Num training images:  103518\n",
            "Num test images:  3365\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oln7OBSOAoX3",
        "colab_type": "text"
      },
      "source": [
        "Whenever you initialize the batch of images, it is on the CPU for computation by default. The function torch.cuda.is_available() will check whether a GPU is present. If CUDA is present, .device(\"cuda\") will route the tensor to the GPU for computation.\n",
        "\n",
        "The device will use CUDA with a single GPU processor. This will make our calculations faster. If you have a CPU in your system, no problem. You can use Google Colab, which provides free GPU."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_xo8cr93-tgS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "b42e50c9-ee27-4b2d-8f44-8cc20ddf6f63"
      },
      "source": [
        "# check if CUDA is available\n",
        "train_on_gpu = torch.cuda.is_available()\n",
        "\n",
        "if not train_on_gpu:\n",
        "    print('CUDA is not available.  Training on CPU ...')\n",
        "else:\n",
        "    print('CUDA is available!  Training on GPU ...')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CUDA is available!  Training on GPU ...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fIxN9SP8AyGH",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "In the code below, `dataloader` combines a dataset and a sampler and provides an iterable over the given dataset. `dataset()` indicates which dataset to load form the available data. For details, [read this documentation](https://pytorch.org/docs/stable/data.html#module-torch.utils.data).\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z74c-DT1EBYe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# define dataloader parameters\n",
        "batch_size = 128\n",
        "num_workers=0\n",
        "\n",
        "# prepare data loaders\n",
        "train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, \n",
        "                                           num_workers=num_workers, shuffle=True)\n",
        "test_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size, \n",
        "                                          num_workers=num_workers, shuffle=True)\n",
        "loaders_scratch = {\n",
        "    'train': train_loader,\n",
        "    'test': test_loader\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TMtf_-mhAzo3",
        "colab_type": "text"
      },
      "source": [
        "# Visulazing the data..\n",
        "\n",
        "Note: The output of data visualisation is hidden due to it's nature."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0TWScgqjsKEK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#encoder and decoder to convert classes into integer\n",
        "decoder = {}\n",
        "for i in range(len(classes)):\n",
        "    decoder[classes[i]] = i\n",
        "encoder = {}\n",
        "for i in range(len(classes)):\n",
        "    encoder[i] = classes[i]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mvbNylTdsnDd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "inv_normalize =  transforms.Normalize(\n",
        "    mean=[-0.4302/0.2361, -0.4575/0.2347, -0.4539/0.2432],\n",
        "    std=[1/0.2361, 1/0.2347, 1/0.2432]\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9kZURFwRr_5E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "#plotting rondom images from dataset\n",
        "n_figures = 9 # Number of images in plot\n",
        "def class_plot( data , encoder ,inv_normalizen_figures = 12):\n",
        "    n_row = int(n_figures/3)\n",
        "    fig,axes = plt.subplots(figsize=(14, 10), nrows = n_row, ncols=3)\n",
        "    for ax in axes.flatten():\n",
        "        a = random.randint(0,len(data))\n",
        "        (image,label) = data[a]\n",
        "        label = int(label)\n",
        "        l = encoder[label]\n",
        "        image = inv_normalize(image)\n",
        "        image = image.numpy().transpose(1,2,0)\n",
        "        im = ax.imshow(image)\n",
        "        ax.set_title(l)\n",
        "        ax.axis('off')\n",
        "    plt.show()\n",
        "class_plot(train_data,encoder,inv_normalize)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nz3FxxulBKLQ",
        "colab_type": "text"
      },
      "source": [
        "#Transfer Learning with Pytorch\n",
        "\n",
        "The main aim of transfer learning (TL) is to implement a model quickly. To solve the current problem, instead of creating a DNN (dense neural network) from scratch, the model will transfer the features it has learned from the different dataset that has performed the same task. This transaction is also known as knowledge transfer.\n",
        "\n",
        "<img src=\"https://i.imgur.com/3sx8Y3i.png\" width = \"600\"/>\n",
        "\n",
        "Why vgg-16?\n",
        "\n",
        "VGG16 significantly outperforms the previous generation of models in the ILSVRC-2012 and ILSVRC-2013 competitions. The VGG16 result is also competing for the classification task winner (GoogLeNet with 6.7% error) and substantially outperforms the ILSVRC-2013 winning submission Clarifai, which achieved 11.2% with external training data and 11.7% without it. Concerning the single-net performance, VGG16 architecture achieves the best result (7.0% test error), outperforming a single GoogLeNet by 0.9%.\n",
        "\n",
        "![alt text](https://neurohive.io/wp-content/uploads/2018/11/Capture-1-770x345.jpg)\n",
        "[Source](https://neurohive.io/en/popular-networks/vgg16/)\n",
        "\n",
        "\n",
        "The Pytorch API calls a pre-trained model of vgg-16 by using models.vgg16(pretrained=True), the function from TorchVision's model library. VGG-16 architecture is described below.\n",
        "\n",
        "<img src=\"https://neurohive.io/wp-content/uploads/2018/11/vgg16-neural-network.jpg\" width = \"600\"/>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "un6P_1F8EG7A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "net = models.vgg16(pretrained=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ypffh-IQFGpy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Freeze training for all \"features\" layers\n",
        "for param in net.features.parameters():\n",
        "    param.requires_grad = False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xuyk-qADFLJ5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "30bf9432-49d6-4c3b-99b3-fa1bb555b310"
      },
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "n_inputs = net.classifier[6].in_features\n",
        "\n",
        "# add last linear layer\n",
        "# new layers automatically have requires_grad = True\n",
        "last_layer = nn.Linear(n_inputs, len(classes))\n",
        "\n",
        "net.classifier[6] = last_layer\n",
        "\n",
        "# if GPU is available, move the model to GPU\n",
        "if train_on_gpu:\n",
        "    net.cuda()\n",
        "\n",
        "# check to see that your last layer produces the expected number of outputs\n",
        "print(net.classifier[6].out_features)\n",
        "#print(net)\n",
        "\n",
        "# after completing your model, if GPU is available, move the model to GPU\n",
        "if train_on_gpu:\n",
        "    net.cuda()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rYOjWpfdG6WW",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "## Specifying Loss & Optimizer:\n",
        "\n",
        "Loss function and optimization go hand-in-hand. Loss function checks whether the model is moving in the correct direction and making progress, whereas optimization improves the model to deliver accurate results.\n",
        "Select any one optimizer algorithm available in the [torch.optim](https://pytorch.org/docs/master/optim.html) package. The optimizers have some elements of the gradient descent.\n",
        "\n",
        "Also, setting the hyperparameters below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aowSRIgvGx2G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "# specify loss function (categorical cross-entropy)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# specify optimizer (stochastic gradient descent) and learning rate = 0.001\n",
        "optimizer = optim.SGD(net.classifier.parameters(), lr=0.001, momentum=0.9)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F5pBMdSCHBYB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "n_epochs = 4\n",
        "print_every = 10\n",
        "valid_loss_min = np.Inf\n",
        "val_loss = []\n",
        "val_acc = []\n",
        "train_loss = []\n",
        "train_acc = []\n",
        "total_step = len(train_loader)\n",
        "\n",
        "    ###################\n",
        "    # train the model #\n",
        "    ###################\n",
        "\n",
        "for epoch in range(1, n_epochs+1):\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total=0\n",
        "    print(f'Epoch {epoch}\\n')\n",
        "    for batch_idx, (data_, target_) in enumerate(train_loader):\n",
        "        data_, target_ = data_.to(device), target_.to(device)\n",
        "        optimizer.zero_grad() # clear-the-gradients-of-all-optimized-variables\n",
        "        outputs = net(data_) # forward-pass: compute-predicted-outputs-by-passing-inputs-to-the-model\n",
        "        loss = criterion(outputs, target_) # calculate-the-batch-loss\n",
        "        loss.backward() # backward-pass: compute-gradient-of-the-loss-wrt-model-parameters\n",
        "        optimizer.step() # perform-a-ingle-optimization-step (parameter-update)\n",
        "\n",
        "        running_loss += loss.item() # update-training-loss\n",
        "        _,pred = torch.max(outputs, dim=1)\n",
        "        correct += torch.sum(pred==target_).item()\n",
        "        total += target_.size(0)\n",
        "        if (batch_idx) % 20 == 0:\n",
        "            print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}' \n",
        "                   .format(epoch, n_epochs, batch_idx, total_step, loss.item()))\n",
        "    train_acc.append(100 * correct / total)\n",
        "    train_loss.append(running_loss/total_step)\n",
        "    print(f'\\ntrain-loss: {np.mean(train_loss):.4f}, train-acc: {(100 * correct/total):.4f}')\n",
        "    batch_loss = 0\n",
        "    total_t=0\n",
        "    correct_t=0\n",
        "\n",
        "    ######################    \n",
        "    # validate the model #\n",
        "    ######################\n",
        "\n",
        "    with torch.no_grad():\n",
        "        net.eval()\n",
        "        for data_t, target_t in (test_loader):\n",
        "            data_t, target_t = data_t.to(device), target_t.to(device)\n",
        "            outputs_t = net(data_t) # forward-pass: compute-predicted-outputs-by-passing-inputs-to-the-model\n",
        "            loss_t = criterion(outputs_t, target_t) # calculate-the-batch-loss\n",
        "            batch_loss += loss_t.item() # update-batch_loss\n",
        "            _,pred_t = torch.max(outputs_t, dim=1)\n",
        "            correct_t += torch.sum(pred_t==target_t).item()\n",
        "            total_t += target_t.size(0)\n",
        "        val_acc.append(100 * correct_t/total_t)\n",
        "        val_loss.append(batch_loss/len(test_loader))\n",
        "        network_learned = batch_loss < valid_loss_min\n",
        "        print(f'validation loss: {np.mean(val_loss):.4f}, validation acc: {(100 * correct_t/total_t):.4f}\\n')\n",
        "\n",
        "        \n",
        "        if network_learned:\n",
        "            valid_loss_min = batch_loss\n",
        "            torch.save(net.state_dict(), 'vgg16a.pt')\n",
        "            print('Improvement-Detected, save-model')\n",
        "    net.train()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0OFeAMdf3iR6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size, \n",
        "                                          num_workers=num_workers, shuffle=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9inxX_4UMb1R",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "outputId": "e48867d0-aebc-4d4f-e9c3-a5a0491c7132"
      },
      "source": [
        "def test(loaders, model, criterion, train_on_gpu):\n",
        "\n",
        "    # monitor test loss and accuracy\n",
        "    test_loss = 0.\n",
        "    correct = 0.\n",
        "    total = 0.\n",
        "\n",
        "    for batch_idx, (data, target) in enumerate(loaders['test']):\n",
        "        # move to GPU\n",
        "        if train_on_gpu:\n",
        "            data, target = data.cuda(), target.cuda()\n",
        "        # forward pass: compute predicted outputs by passing inputs to the model\n",
        "        output = model(data)\n",
        "        # calculate the loss\n",
        "        loss = criterion(output, target)\n",
        "        # update average test loss \n",
        "        test_loss = test_loss + ((1 / (batch_idx + 1)) * (loss.data - test_loss))\n",
        "        # convert output probabilities to predicted class\n",
        "        pred = output.data.max(1, keepdim=True)[1]\n",
        "        # compare predictions to true label\n",
        "        correct += np.sum(np.squeeze(pred.eq(target.data.view_as(pred))).cpu().numpy())\n",
        "        total += data.size(0)\n",
        "            \n",
        "    print('Test Loss: {:.6f}\\n'.format(test_loss))\n",
        "\n",
        "    print('\\nTest Accuracy: %2f%% (%2d/%2d)' % (100. * correct / total, correct, total))\n",
        "\n",
        "# call test function    \n",
        "test(loaders_scratch, net, criterion, train_on_gpu)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test Loss: 0.149996\n",
            "\n",
            "\n",
            "Test Accuracy: 94.442793% (3178.000000/3365.000000)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C8auqEd60exz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from PIL import Image\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "def load_input_image(img_path):    \n",
        "    image = Image.open(img_path).convert('RGB')\n",
        "    prediction_transform = transforms.Compose([transforms.Resize(size=(224, 224)),\n",
        "                                     transforms.ToTensor(), \n",
        "                                     standard_normalization])\n",
        "\n",
        "    # discard the transparent, alpha channel (that's the :3) and add the batch dimension\n",
        "    image = prediction_transform(image)[:3,:,:].unsqueeze(0)\n",
        "    return image"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vaao8vU16wmv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "loaders_transfer = loaders_scratch.copy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DS1LX-8E6cL3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class_names = [item[4:].replace(\"_\", \" \") for item in loaders_transfer['train'].dataset.classes]\n",
        "\n",
        "\n",
        "def predict_nsfw(model, class_names, img_path):\n",
        "    # load the image and return the predicted breed\n",
        "    img = load_input_image(img_path)\n",
        "    model = model.cpu()\n",
        "    model.eval()\n",
        "    idx = torch.argmax(model(img))\n",
        "    return class_names[idx]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FkmZmlrw_Qeb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(net)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B4lHvyofAv-e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "torch.save(net.state_dict(), 'vgg16_c.pth')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fmY1KssyA9o1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "673e6a6a-e4d4-416b-9c16-e78e96777a92"
      },
      "source": [
        "net.load_state_dict(torch.load('vgg16_c.pth'))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 85
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xVKZIL7QQ_Pj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.plot(train_loss, label='Training loss')\n",
        "plt.plot(val_loss, label='Validation loss')\n",
        "plt.legend(frameon=False)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kQHZLoR7Bldm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def test(loaders, model, criterion, train_on_gpu):\n",
        "\n",
        "    # monitor test loss and accuracy\n",
        "    test_loss = 0.\n",
        "    correct = 0.\n",
        "    total = 0.\n",
        "\n",
        "    for batch_idx, (data, target) in enumerate(loaders['test']):\n",
        "        # move to GPU\n",
        "        if train_on_gpu:\n",
        "            data, target = data.cuda(), target.cuda()\n",
        "        # forward pass: compute predicted outputs by passing inputs to the model\n",
        "        output = model(data)\n",
        "        # calculate the loss\n",
        "        loss = criterion(output, target)\n",
        "        # update average test loss \n",
        "        test_loss = test_loss + ((1 / (batch_idx + 1)) * (loss.data - test_loss))\n",
        "        # convert output probabilities to predicted class\n",
        "        pred = output.data.max(1, keepdim=True)[1]\n",
        "        # compare predictions to true label\n",
        "        correct += np.sum(np.squeeze(pred.eq(target.data.view_as(pred))).cpu().numpy())\n",
        "        total += data.size(0)\n",
        "            \n",
        "    print('Test Loss: {:.6f}\\n'.format(test_loss))\n",
        "\n",
        "    print('\\nTest Accuracy: %2f%% (%2d/%2d)' % (100. * correct / total, correct, total))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VyH1uRboAaDF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "outputId": "18a5bf8f-60ef-43fd-ec68-5ea4a5644587"
      },
      "source": [
        "# call test function    \n",
        "test(loaders_transfer, net, criterion, train_on_gpu)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test Loss: 0.147624\n",
            "\n",
            "\n",
            "Test Accuracy: 94.650817% (3185/3365)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iNSmRD-EAeWP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def performance_matrix(true,pred):\n",
        "    precision = metrics.precision_score(true,pred,average='macro')\n",
        "    recall = metrics.recall_score(true,pred,average='macro')\n",
        "    accuracy = metrics.accuracy_score(true,pred)\n",
        "    f1_score = metrics.f1_score(true,pred,average='macro')\n",
        "    print('Confusion Matrix:\\n',metrics.confusion_matrix(true, pred))\n",
        "    print('Precision: {} Recall: {}, Accuracy: {}: ,f1_score: {}'.format(precision*100,recall*100,accuracy*100,f1_score*100))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jmGM3EzpBs05",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "! cp '/content/vgg16_c.pth' '/content/drive/My Drive/phase_2_chkpts'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Kc3SeGJQuZ1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import sklearn"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xswc10l0DcGG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "5a8e3d48-77cb-4787-9a38-53c5a91bd37b"
      },
      "source": [
        "nb_classes = 2\n",
        "\n",
        "confusion_matrix = torch.zeros(nb_classes, nb_classes)\n",
        "with torch.no_grad():\n",
        "    for i, (inputs, classes) in enumerate(loaders_transfer['test']):\n",
        "        inputs = inputs.to(device)\n",
        "        classes = classes.to(device)\n",
        "        outputs = net(inputs)\n",
        "        _, preds = torch.max(outputs, 1)\n",
        "        for t, p in zip(classes.view(-1), preds.view(-1)):\n",
        "                confusion_matrix[t.long(), p.long()] += 1\n",
        "\n",
        "print(confusion_matrix)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[1600.,   69.],\n",
            "        [ 127., 1569.]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A86euVfSGIgv",
        "colab_type": "text"
      },
      "source": [
        "### per-class confusion matrix"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tzqG0iklFaOv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "8916d8cc-7168-46b7-c98f-398c41f9e914"
      },
      "source": [
        "print(confusion_matrix.diag()/confusion_matrix.sum(1))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([0.9605, 0.9228])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4pjJEQ1wHkFW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import classification_report, confusion_matrix"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0fHkElX7H0FE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cm = confusion_matrix.numpy().astype(int)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vNDzyPaGJamV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df = pd.DataFrame(data=cm, index=[\"nsfw\", \"sfw\"], columns=[\"nsfw\", \"sfw\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3-MbTki-Js6f",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 396
        },
        "outputId": "3d874a5f-d683-42cd-9491-ea3b260a4279"
      },
      "source": [
        "plt.figure(figsize=(6,6))\n",
        "sns.heatmap(df, annot=True, annot_kws={\"size\": 10},fmt=\"d\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7fba69b865c0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 132
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x432 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SgT4eG1EJzkg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}